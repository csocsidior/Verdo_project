\chapter{System identification design}
\label{identification_design}

\emph{In this chapter, the Multi-inlet, Multi-WT model, which has been derived based on first principles in \chapref{system_modelling}, is reformulated such that it is suitable for system identification. First, a model structure is selected, then a Neural Network(NN) based Radial Basis Function(RBF) model is presented. In the first case, the identification is carried out on a simple example, then on the Randers WSS EPANET model. In the second case, the identification is carried out based on measurements from the real-world network.}

\section{Model structure of the Multi-inlet, Multi-WT system}
\label{model_structure_of_the_multi_inlet_multi_WT_system}

In \chapref{system_modelling}, the model of a multi-inlet WSS with the extension of multiple WTs has been derived. The presented model has been given by a non-linear SS representation, which consists of the equations describing the dynamics, the outputs and the constraints on the network. As the model derivation is based on first principles, some insight into the structure and the input-output relation is available. 

\subsection{Output equation}
\label{output_eq_identification}

The model is going to be utilized for identification purposes, therefore let us recall the governing equations. The output vector $\bar{p}_{\mathcal{K}}$ of the inlet pressures is given in discrete form such that

\begin{equation}
  \label{recall_output_eq}
  \bar{p}_{\mathcal{K},k} = K^T \bar{H}^{-T}_{\mathcal{T}}f_{\mathcal{T}}(A_2 q_{\mathcal{C},k} + A_3 K \bar{d}_{\mathcal{K},k} - A_3 D v_{\mathcal{D}} \sigma_k) - K^T\bar{H}^{-T}_{\mathcal{T}}\hat{H}^{T}_{\mathcal{T}} (\hat{p}_k + \hat{h}) - K^T\bar{h} ,
\end{equation} 

\begin{minipage}[t]{0.4\textwidth}
where\\
\hspace*{8mm} $A_2 = -\bar{H}^{-1}_{\mathcal{T}} \bar{H}_{\mathcal{C}} $, \vspace*{1.5mm}\\
\hspace*{8mm} $A_3 = \bar{H}^{-1}_{\mathcal{T}}$.
\end{minipage}

Furthermore, let us recall the constraint on $q_\mathcal{C}$, and rewrite it in discrete-time form such that

 \begin{equation}
\label{recall_constraint eq}
f_{\mathcal{C}}(q_{\mathcal{C},k}) - A_1(\hat{p}_k + \hat{h}) + A_2^T f_{\mathcal{T}}(A_2 q_{\mathcal{C},k} + A_3 K \bar{d}_{\mathcal{K},k} - A_3 D v_{\mathcal{D}} \sigma_k) = 0,
\end{equation} 

with $\bar{d}_{\mathcal{K},k}$ inlet flows, $\sigma_k$ total demand, $v_{\mathcal{D}}$ distribution parameter, $q_{\mathcal{C},k}$ flows in set $\mathcal{C}$, $\hat{p}_k$ pressures in the WTs, $\hat{h}$ elevations of the WTs and $K^T\bar{h} = \bar{h}_{\mathcal{K}} $ the elevations of the pumping stations. The corresponding values of the pressures and flows in the network are evaluated at each time step $k$. 

It is important to point out that the model in \chapref{system_modelling} has been derived in a general manner, taking into account that $v_{\mathcal{D}}$ is a time-varying distribution parameter of the demands. However, in the further description, let us restrict ourselves and assume that $v_{\mathcal{D}}$ is constant. This assumption is beneficial from the system point of view, as the total consumption $\sigma_k$ can be represented simply by the sum of the hourly demand variations $1^T \bar{d}_{\mathcal{D}}$. From the technical point of view, the identification becomes less complex, as in this case $v_{\mathcal{D}}$ is a linear constant parameter. In \eqref{recall_constraint eq} and \eqref{recall_output_eq} this assumption is already taken into account, therefore $v_{\mathcal{D}}$ does not have any time index. 

The constraint on $q_\mathcal{C}$ in \eqref{recall_constraint eq} is given by an implicit expression for which analytical soulution has not been derived. Therefore, the constraint cannot be substituted explicitly into \eqref{recall_output_eq}, but can be given in the following following implicit form in \eqref{qc_abstraction}.

 \begin{equation}
\label{qc_abstraction}
q_{\mathcal{C},k} = q_\mathcal{C} \big( (\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k\big).
\end{equation} 

It is shown in \eqref{qc_abstraction} that the $q_\mathcal{C}$ flows in the network depend on the same physical measures, i.e. the same variables as the outputs $\bar{p}_{\mathcal{K}}$. By substituting \eqref{qc_abstraction} into \eqref{recall_output_eq}, we get the following output equation

\vspace{-4mm}
\begin{align}
  \label{recall_output_eq_2}
      \bar{p}_{\mathcal{K},k} + \bar{h}_{\mathcal{K}} = & \nonumber K^T \bar{H}^{-T}_{\mathcal{T}}f_{\mathcal{T}}[A_2 q_{\mathcal{C}}\big ((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k \big) + A_3 K \bar{d}_{\mathcal{K},k} - A_3 D v_{\mathcal{D}} \sigma_k]   \\ &  - K^T\bar{H}^{-T}_{\mathcal{T}}\hat{H}^{T}_{\mathcal{T}} (\hat{p}_k + \hat{h}) .
\end{align}

\vspace{-4mm}
In \eqref{recall_output_eq_2}, the total head in the pumping stations $(\bar{p}_{\mathcal{K},k} + \bar{h}_{\mathcal{K}})$ is given by the expression on the right-hand side. Let us write \eqref{recall_output_eq_2} in a form where the non-linear expression on the right-hand side is replaced with a non-linear function $\tilde{f}_1(\cdot)$,which has an unknown structure but has the same variables in the argument. Thus, a reformulated output equation can be given such that 

 \begin{equation}
  \label{recall_output_eq_3}
     \tilde{y}_k = \bar{p}_{\mathcal{K},k} + \bar{h}_{\mathcal{K}} = \tilde{f}_1 \big((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k\big) + \tilde{a}_1 (\hat{p}_k + \hat{h}), 
\end{equation} 

The static model described in \eqref{recall_output_eq_3} is a mapping defined by the non-linear function $\tilde{f}_1$ and the linear term, which maps the input set, $u = \{(\hat{p}_k + \hat{h}), \bar{d}_{\mathcal{K}}, \sigma \}$ to the outputs $\tilde{y} = \{ \bar{p}_{\mathcal{K},k} + \bar{h}_{\mathcal{K}} \}$. In the input set, the total consumption can be calculated according to the mass-balance in the whole network such that

\begin{equation}
\label{massbalance_identification}
 \sigma_k = 1^T \hat{d}_k + 1^T \bar{d}_{\mathcal{K},k}.
\end{equation}

 In \eqref{massbalance_identification}, we assume that the flows in the WTs are measured. 

 % Nevertheless, the input and output set together forms the TS $\mathcal{D}_1 = \{u_i ; \tilde{y}_i\}_{i = 1,2, ..., N}$, which can be utilized to carry out the identification on the model.

 \subsection{State equation}
\label{state_eq_identification} 

The state equation is a first-order system of ODEs, which has been formulated on the pressures $\hat{p}$ in the WTs. In order to give a discretization for the approximate solution of the ODEs, Euler-method is applied. The Euler-method is the simplest Runge-Kutte method, which provides an acceptable precision for our problem\cite{chicone2006ordinary}. Thus, the discretized state equation yields as follows

\begin{equation}
  \label{WT_matrixform_final_discrete}
\Lambda \frac{1}{T_s} (\hat{p}_{k+1} - \hat{p}_k)  = - (\hat{H}_{\mathcal{C}} - \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}}\bar{H}_{\mathcal{C}})  q_{\mathcal{C},k} - \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}} K \bar{d}_{\mathcal{K},k} + \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}} D v_{\mathcal{D}} \sigma_k.
\end{equation}

Substituting the constraint on $q_{\mathcal{C}}$ into \eqref{WT_matrixform_final_discrete}, and expressing the approximation of the derivative term on the left-hand side, the following yields

\vspace{-4mm}
\begin{align}
\label{WT_matrixform_final_discrete1}
\nonumber  \hat{p}_{k+1} - \hat{p}_k  =& T_s \Lambda^{-1} \big[- (\hat{H}_{\mathcal{C}} - \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}}\bar{H}_{\mathcal{C}})  q_{\mathcal{C},k}\big ((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k \big) \\ & - \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}} K \bar{d}_{\mathcal{K},k} + \hat{H}_{\mathcal{T}} \bar{H}^{-1}_{\mathcal{T}} D v_{\mathcal{D}} \sigma_k \big].
\end{align}
\vspace{-4mm}

\begin{minipage}[t]{0.20\textwidth}
where\\
\hspace*{8mm} $T_s$
\end{minipage}
\begin{minipage}[t]{0.68\textwidth}
\vspace*{2mm}
 is the sampling time.
\end{minipage}
\begin{minipage}[t]{0.10\textwidth}
\vspace*{2mm}
\textcolor{White}{te}$\unit{h}$
\end{minipage} 

The state equation in \eqref{WT_matrixform_final_discrete} describes the linear combination of the the flows $q_\mathcal{C}$, the total head in the WTs $(\hat{p}_k + \hat{h})$ and the total consumption $\sigma$. However, by substituting the $q_\mathcal{C}$ flows with their implicit non-linear expression, the structure of the state equation is not a linear combination of the corresponding signals anymore. Therefore, let us write \eqref{WT_matrixform_final_discrete1} in a form where the non-linear and linear terms are described by a non-linear $\tilde{f}_2(\cdot)$ function with unknown structure and linear terms with parameters, respectively. Thus, a reformulated state equation can be given such that

 \begin{equation}
  \label{WT_matrixform_final_discrete2}
     \hat{p}_{k+1} - \hat{p}_k = \tilde{f}_2 \big((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k\big) + \tilde{a}_2 \bar{d}_{\mathcal{K},k} + \tilde{a}_3 \sigma_k,
\end{equation} 


where $\tilde{f}_2$ is a non-linear function, $\tilde{a}_2$ and $\tilde{a}_3$ are parameters of the inlet flows $\bar{d}_{\mathcal{K},k}$ and total consumption $\sigma_k$. 


\section{RBFNN model of the Multi-inlet,Multi-WT system}
\label{RBFNN_model_multi_inlet_multi_WT_sys} 

As a result of substituting the constraints on the flows $q_{\mathcal{C},k}$, the system description has been reduced to a non-linear SS problem with state equation given in \eqref {WT_matrixform_final_discrete2} and output equation in \eqref {recall_output_eq_3}. The complete identification model is summarized in \eqref{identification_model}. 

\begin{equation}
\begin{cases}
  \label{identification_model}
    \hat{p}_{k+1} - \hat{p}_k = \tilde{f}_2 \big((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k\big) + \tilde{a}_2 \bar{d}_{\mathcal{K},k} + \tilde{a}_3 \sigma_k,\\
  \tilde{y}_k  = \tilde{f}_1 \big((\hat{p}_k + \hat{h}),\bar{d}_{\mathcal{K},k}, \sigma_k\big) + \tilde{a}_1 (\hat{p}_k + \hat{h}).
  \end{cases}
\end{equation} 

The main goal of the system identification is to find a realization of the functions $\tilde{f}_1(\cdot)$ and $\tilde{f}_2(\cdot)$, furthermore to find the parameters $\tilde{a}_1$, $\tilde{a}_2$ and $\tilde{a}_3$. Therefore, the parameters need to be identified, such that the model is able to reproduce the approximate of the state derivatives $(\hat{p}_{k+1} - \hat{p}_k)$ and the outputs $\tilde{y}_k$ from any input set $u = \{(\hat{p}_k + \hat{h}), \bar{d}_{\mathcal{K}}, \sigma \}$ within the operating regions where we are mapping from.

The identification model shown in \eqref{identification_model} is an abstraction of the first principle model derived in \chapref{system_modelling}. For the constraint on $q_{\mathcal{C},k}$, existence has been implied, however exact structure has not been given. I.e. we know that the relationship exists but an analytical first principle solution for $q_{\mathcal{C},k}$ has not been derived. Therefore, by substituting the implicit expression of the constraint into the state and output equations, some of the insights on the structure of the model are lost. Thus, it is crucial to put a structure on the non-linear functions $\tilde{f}_1(\cdot)$ and $\tilde{f}_2(\cdot)$ in \eqref{identification_model}. 

From a technical point of view, it is beneficial to describe the system by a linear-in-the-parameters model. By restricting ourselves such that the structure of both functions $\tilde{f}_1(\cdot)$ and $\tilde{f}_2(\cdot)$ are linear in the parameters, the parameters of the model can be estimated by simple linear optimization methods, such as Least Squares(LS). For any linear optimization, only the parameters have to enter linearly, as the inputs can depend on any non-linear way on the input data sets. 

By making the restriction on $\tilde{f}_1(\cdot)$ and $\tilde{f}_2(\cdot)$, the two non-linear terms will be approximated by some non-linear functions in both the state and output equations of the model. The tools for carrying out such identification procedure leads to the discussion of basis functions and neural networks, which are discussed in \appref{neural_networks} in detail. 

\newpage

As it was concluded, the output is governed by the non-linear function $\tilde{f}_1$, which maps the inputs $\bar{d}_{\mathcal{K}}$, $\sigma$ and $\hat{h}_t $ to the outputs $\tilde{y}$. This static input-output system can be represented with a RBFNN, shown in \figref{fig:nn_output}.
the interpretation of the Multi-inlet, Multi-WT system in the context of NNs is considered. 

  %NN model of the output eq.
 \begin{figure}[H]
 \centering
 %\includegraphics[width=0.35\textwidth]{report/pictures/missingfigure}
 \input{report/tikz/output_NN1.tex} 
  \vspace{-7mm}
 \caption{NN model of the output equation.}
 \label{fig:nn_output}
 \end{figure}

 \vspace{-3mm}

 In \figref{fig:nn_output}, the first layer consists of the inputs, the hidden layer forms the set of RBFs and, as there are two outputs in the network. The output layer consists of two output neurons which linearly combine the weighted RBFs. Therefore, the output equation written in RBFNN formulation can be given such that 

  \begin{equation}
\label{NN_output_eq1}
\tilde{y}_1 = \sum_{i = 1}^M w_i \phi_i(u) +  w_{0,1},
\end{equation}

where $u = (\bar{d}_{\mathcal{K}} \ \sigma \ \hat{h}_t )^T$. Using vector notation, \eqref{NN_output_eq1} can be rewritten such that

  \begin{equation}
\label{NN_output_vector1}
\tilde{y} = \theta^T_{\tilde{y}} 
          \begin{pmatrix}
           \phi_1(u) \\[1pt]
           \phi_2(u) \\[1pt]
           \vdots \\[1pt]
           \phi_M(u)\\[3pt]
           1 
         \end{pmatrix}
         =
         \theta^T_{\tilde{y}} \chi_{\tilde{y}},
\end{equation}

where $\theta_{\tilde{y}}$ is the regression matrix, including the output weights $w_i$ and the biases $w_0$. Furthermore, $\theta$ has as many columns as the number of inputs. $\chi_{\tilde{y}}$ is called the regressor vector, or in NN context the input vector, consisting of the basis functions and the vector $1 \in \: \mathbb{R}^{c} $, where $c$ is the number of outputs. 

The state equation is governed by the non-linear function $\tilde{f}_2$ and furthermore, the states $\hat{p}$ and the elevation of the WTs $\hat{h}$ enter the system linearly. As a consequence of this, the measured data of the states have a strong linear component. In the NN representation this is taken into account such that skip-layer connections are introduced. These extra connections in the NN model are skipping the hidden layer, thereby contribute linearly to the outputs. The illustration of such RBFNN is shown in \figref{fig:nn_state}.

   %NN model of the state eq.
 \begin{figure}[H]
 \centering
 %\includegraphics[width=0.35\textwidth]{report/pictures/missingfigure}
 \input{report/tikz/output_NN.tex} 
 \caption{NN model of the state equation.}
 \label{fig:nn_state}
 \end{figure}

 \vspace{-3mm}

 In \figref{fig:nn_state}, although the input neurons for the skip-layer connections are placed in line with the hidden layer, they are considered as simple inputs, effecting the output layer by the parameters $a_1$ and $a_2$. Nevertheless, the identification is treated as a black-box model, but since insight is given about the network architecture, the NN model can be better matched to the problem by utilizing these skip-layer connections. Furthermore, in \figref{fig:nn_state}, the inputs are all present values and the outputs are the future or predicted values of the states $\hat{p}_{k+1}$. $\hat{h}$ does not have a time step index, as this value is the elevation of WTs, which is time-invariant. 

 Using vector notation, the NN model for the state equation can be given in the form as shown in \eqref{NN_state_vector2}

  \begin{equation}
\label{NN_state_vector2}
\hat{p}_{k+1} = \theta^T_{\hat{p}} 
          \begin{pmatrix}
           \phi_1(u_k) \\[1pt]
           \phi_2(u_k) \\[1pt]
           \vdots \\[1pt]
           \phi_M(u_k)\\[3pt]
           \hat{p}_k \\
           \hat{h}\\
           1 
         \end{pmatrix}
         =
         \theta^T_{\hat{p}} \chi_{\hat{p}},
\end{equation}

where $\theta^T_{\hat{p}}$ is the regression matrix, including the output weights $w_i$, the skip-layer connection weights $a_i$ and the biases $w_0$. The regressor vector $\chi_{\hat{p}}$ consists of the basis functions, the linear inputs $\hat{p}_k$ and $\hat{h}$ and the vector $1 \in \: \mathbb{R}^{l} $, where $l$ is the number of states.




