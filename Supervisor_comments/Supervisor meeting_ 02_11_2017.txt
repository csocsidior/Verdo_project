Supervisor meeting 

flow out of the tanks -> measured? 
linear dynamics
polynomial based tool for system id. 
RBF NN - > radial basis functions 
Bernstein polynomials if we choose splines as radial basis functions 
nntool-> matlab 
nntool
output-> from EPANET model

change the output equation to P_k ->OK
\sigma is an unknown variable in the real world
the dynamic equations are non-recursive -> not necessary to use recursive NNs 

21st -> Tuesday 10.000 supervisor meeting. 



Supervisor meeting with Jan and Tom

If too many neurons -> overfit of the function -> stop when it's a reasonable amount
Since there is no measurement noise, everything is assumed to be deterministic in the model
-> There is no need of training and validation data, training data is sufficient, as the outcome for the
two identification procedure would be the same
A_2^T mistake !!!!!











Supervisor meeting 21st 

RBF -> can mimic |q|q -> polynomials might not ()
Basis function -> more insight than neural networks
Radial basis functions -> exponential, affine + linear transformations by changing 
LW and B2 - >output weights

input data -> length-> until reaches steady-state or

Correct mistakes in the discussion notes ->

next tuesday 10.00



Practical applications of NNs most often employ supervised learning. For supervised learning, you must provide
 training data that includes both the input and the desired result (the target value). After successful training,
 you can present input data alone to the NN (that is, input data without the desired result), and the NN will compute
 an output value that approximates the desired result. However, for training to be successful, you may need lots of training
 data and lots of computer time to do the training. In many applications, such as image and text processing, you will have to do
 a lot of work to select appropriate input data and to code the data as numeric values.

In practice, NNs are especially useful for classification and function approximation/mapping problems which
 are tolerant of some imprecision, which have lots of training data available, but to which hard and fast rules
 (such as those that might be used in an expert system) cannot easily be applied. Almost any finite-dimensional 
vector function on a compact set can be approximated to arbitrary precision by feedforward NNs (which are the type 
most often used in practical applications) if you have enough data and enough computing resources.


https://se.mathworks.com/matlabcentral/answers/73346-how-to-use-different-transfer-functions-within-the-same-layer-of-a-neural-network
https://se.mathworks.com/matlabcentral/fileexchange/52580-radial-basis-function-neural-networks--with-parameter-selection-using-k-means-
http://dungba.org/linear-regression-with-neural-network/

include a linear function through skip-layer connections.


11
down vote
In theory, skip-layer connections should not improve on the network performance. But, since complex networks are hard
 to train and easy to overfit it may be very useful to explicitly add this as a linear regression term, when you know 
that your data has a strong linear component. This hints the model in a right direction... In addition, this is more 
interpretable since it presents your model as linear + perturbations, unraveling a bit of a structure behind the network, 
which is usually seen merely as a black box


The real advantage is that it makes estimating good values for the weights easier if the network architecture is well 
matched to the problem, and you may be able to use a smaller network and obtain better generalisation performance.

scale = 1,
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    thick,
    inner sep=8pt
    },
  nodes in empty cells,
  column sep=1.5cm,
  row sep=-8pt
  },
>=latex
]

\matrix[net] (mat)
{
  & |[plain]|  & |[plain]|  \\
 |[plain]| & |[plain]|  \\
|[plain]| & & \\
 |[plain]| & |[plain]| \\
 & |[plain]|   \\
|[plain]| & |[plain]| &  |[plain]|\\
|[plain]| &   &\\
|[plain]|& |[plain]| \\
&  |[plain]|  \\
|[plain]| &   |[plain]| \\
  |[plain]| &  |[plain]| \\
};

\foreach \ai [count=\mi ]in {1,5}
     \draw[thick][<-] (mat-\ai-1) -- node[above] {$u_\mi$} +(-1.5cm,0);
     \draw[thick][<-] (mat-9-1) -- node[above] {$u_{p-1}$} +(-1.5cm,0);
 

\foreach \ai in {1,5,9}
{\foreach \aii  in {3,7}
  \draw[thick][->] (mat-\ai-1) -- (mat-\aii-2) ;
}

  \draw[->] (mat-1-1) -- (mat-3-2) node(){\footnotesize $\phi_1\!(\cdot)$};
  \draw [->] (mat-5-1) -- (mat-7-2) node(){\footnotesize$\phi\!_M\!(\cdot)$};

  \draw[thick][->] (mat-3-2) --node[above]{$w_1$} (mat-7-3)node(){ $\sum$};
  \draw[thick][->] (mat-7-2) --node[above]{$w_j$} (mat-7-3);
  
    \draw[thick][->] (mat-3-2) --node[above]{$w_1$} (mat-3-3)node(){ $\sum$};
  \draw[thick][->] (mat-7-2) --node[below]{$w_j$} (mat-3-3);

  
\draw[thick][->] (mat-6-3) -- node[above] {$\tilde{y}$} +(1.5cm,0);

%COMMENTED
%\draw[thick][<-] (mat-6-3) -- node[right] {$w_0$} +(0,1.5cm);



